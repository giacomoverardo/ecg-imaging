{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "current_path = \"/home/giacomo/ecgi\"#os.getcwd()\n",
    "results_path = \"/home/giacomo/ecgi/results\"\n",
    "\n",
    "# Plot settings\n",
    "labelssize = 14\n",
    "ticksize = 12\n",
    "legendsize = 12\n",
    "colors = [\"#fec44f\", \"#d95f0e\", \"#fff7bc\"]#[\"#66c2a5\", \"#fc8d62\", \"#8da0cb\"]\n",
    "\n",
    "# FMM settings \n",
    "num_features = 260\n",
    "num_waves = 2\n",
    "\n",
    "def get_subfolders_at_depth(folder_path, depth):\n",
    "    #TODO: DOES NOT WORK FOR DEPTH DIFFERENT THAN 1\n",
    "    assert depth==1\n",
    "    # Source: https://stackoverflow.com/questions/7159607/list-directories-with-a-specified-depth-in-python\n",
    "    path = folder_path\n",
    "    path = os.path.normpath(path)\n",
    "    res = []\n",
    "    for root,dirs,files in os.walk(path, topdown=True):\n",
    "        cuurent_depth = root[len(path) + len(os.path.sep)-1:].count(os.path.sep)\n",
    "        if cuurent_depth == depth:\n",
    "            # We're currently two directories in, so all subdirs have depth 3\n",
    "            res += [os.path.join(root, d) for d in dirs]\n",
    "            # dirs[:] = [] # Don't recurse any deeper\n",
    "    # res.pop(0)\n",
    "    return res\n",
    "\n",
    "def get_conf_from_paths(paths):\n",
    "    conf_list = []\n",
    "    for p in paths:\n",
    "        file_path = os.path.join(p,\"conf.yaml\")\n",
    "        # with open(file_path,\"r\") as fp:\n",
    "        cfg = OmegaConf.load(file_path)\n",
    "        cfg[\"path\"] = p\n",
    "        conf_list.append(cfg)\n",
    "    return conf_list\n",
    "\n",
    "def get_completed_conf_files(config_files, log=True):\n",
    "    completed_config_files = []\n",
    "    not_completed_config_files = []\n",
    "    not_completed_experiments = 0\n",
    "    completed_experiments = 0\n",
    "    \n",
    "    for cfg in config_files:\n",
    "        if(cfg[\"completed\"]==True):\n",
    "            completed_config_files.append(cfg)\n",
    "            completed_experiments += 1\n",
    "        else:\n",
    "            not_completed_config_files.append(cfg)\n",
    "            not_completed_experiments += 1\n",
    "    # if(not_completed_experiments>0):\n",
    "    # print(f\"Completed experiments: {completed_experiments} \\nNot completed experiments: {not_completed_experiments}\")\n",
    "    total_experiments = completed_experiments + not_completed_experiments\n",
    "    if(log):\n",
    "        print(f\"Completed experiments: {completed_experiments}/{total_experiments}\")\n",
    "    return completed_config_files,not_completed_config_files\n",
    "\n",
    "def load_json_dict(path):\n",
    "    with open(path) as f:\n",
    "        file = json.load(f)\n",
    "    return file\n",
    "\n",
    "def load_dict_from_paths(paths, file_name):\n",
    "    # Load dicts from paths\n",
    "    dict_list = []\n",
    "    for path in paths:\n",
    "        full_file_name = os.path.join(path,file_name)\n",
    "        mydict = load_json_dict(full_file_name)\n",
    "        dict_list.append(mydict)\n",
    "    return dict_list\n",
    "\n",
    "def load_numpy_from_paths(paths,file_name):\n",
    "    # Load numpy file from paths\n",
    "    np_list = []\n",
    "    for path in paths:\n",
    "        full_file_name = os.path.join(path,file_name)\n",
    "        np_item = np.load(full_file_name)\n",
    "        np_list.append(np_item)\n",
    "    return np_list\n",
    "    \n",
    "def load_json_from_exp(paths, file_name):\n",
    "    # Load json file from paths\n",
    "    roc_list = []\n",
    "    for path in paths:\n",
    "        full_file_name = os.path.join(path,file_name)\n",
    "        roc_dict = json.load(open(full_file_name,\"r\")) \n",
    "        roc_list.append(roc_dict)\n",
    "    return roc_list\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "        \n",
    "def extract_key_from_histories(history_list, max_len=1000, key=\"loss\"):\n",
    "    num_items = len(history_list)\n",
    "    history_matrix = np.NaN * np.ones((num_items,max_len))\n",
    "    for i,h_dict in enumerate(history_list):\n",
    "        h = h_dict[key]\n",
    "        len_h = len(h)\n",
    "        history_matrix[i,:len_h] = h\n",
    "    return history_matrix\n",
    "\n",
    "def extract_auroc_from_roc_list(roc_list):\n",
    "    num_items = len(roc_list)\n",
    "    auroc_vector = np.zeros((num_items))\n",
    "    for i,roc_dict in enumerate(roc_list):\n",
    "        auroc_vector[i] = roc_dict[\"roc_auc\"]\n",
    "    return auroc_vector\n",
    "\n",
    "def save_png_eps_figure(filename):\n",
    "    full_filename = os.path.join(current_path,\"images\",filename)\n",
    "    for extension in [\".png\",\".eps\",\".pdf\"]:\n",
    "        plt.savefig(full_filename+extension,bbox_inches='tight')\n",
    "\n",
    "def get_all_experiments_paths(base_path, model, dataset, optimizer, learning_rate, depth=1):\n",
    "    # Function to get all the result paths for a given model, dataset, split, optimizer, learning rate\n",
    "    lr = str(learning_rate)\n",
    "    folder_path = os.path.join(base_path,f\"{model}_{dataset}\",f\"opt_{optimizer}_lr_{lr}\")\n",
    "    print(folder_path)\n",
    "    return get_subfolders_at_depth(folder_path=folder_path,depth=depth)\n",
    "\n",
    "learning_rates=[1e-3, 5e-4, 1e-4, 5e-5, 1e-5, 5e-6, 1e-6] #(1e-4 1e-5 1e-6) #\n",
    "reconstruction_signal_weights=[1.0, 0.1, 0.01]\n",
    "# seeds=(42) #seeds=(42, 123, 456, 789, 1011)  # Define the seeds\n",
    "seeds=[42, 123, 456, 789, 1011]\n",
    "exp_paths_inria_vae = []\n",
    "exp_paths_latent_lead = []\n",
    "for lr in learning_rates:\n",
    "    exp_paths_inria_vae.extend(get_all_experiments_paths(base_path=results_path,model=\"vae-inria\",dataset=\"inria\",\n",
    "                            optimizer=\"adam\", learning_rate=lr))\n",
    "    exp_paths_latent_lead.extend(get_all_experiments_paths(base_path=results_path,model=\"latent-lead\",dataset=\"inria_gat\",\n",
    "                            optimizer=\"adam\", learning_rate=lr))\n",
    "inria_vae_conf_files = get_conf_from_paths(exp_paths_inria_vae)\n",
    "inria_vae_completed_conf_files, inria_vae_incompleted_conf_files = get_completed_conf_files(inria_vae_conf_files)\n",
    "latent_lead_conf_files = get_conf_from_paths(exp_paths_latent_lead)\n",
    "latent_lead_completed_conf_files, latent_lead_incompleted_conf_files = get_completed_conf_files(latent_lead_conf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_lead_losses_with_removed_leads(exp_path):\n",
    "    removed_leads_range = range(0, 250, 20)\n",
    "\n",
    "    train_losses_dict = {\n",
    "        \"loss\": [],\n",
    "        \"acti_loss\": [],\n",
    "        \"signal_loss\": [],\n",
    "        \"mae_acti_loss\": []\n",
    "    }\n",
    "\n",
    "    test_losses_dict = {\n",
    "        \"loss\": [],\n",
    "        \"acti_loss\": [],\n",
    "        \"signal_loss\": [],\n",
    "        \"mae_acti_loss\": []\n",
    "    }\n",
    "\n",
    "    val_losses_dict = {\n",
    "        \"loss\": [],\n",
    "        \"acti_loss\": [],\n",
    "        \"signal_loss\": [],\n",
    "        \"mae_acti_loss\": []\n",
    "    }\n",
    "\n",
    "    for num_removed_leads in removed_leads_range:\n",
    "        for slice in [\"test\", \"val\"]: #[\"train\", \"test\", \"val\"]\n",
    "            loss_dict = load_json_dict(os.path.join(exp_path, f\"{slice}_loss_{num_removed_leads}\"))\n",
    "            if slice == \"train\":\n",
    "                for key in train_losses_dict:\n",
    "                    train_losses_dict[key].append(loss_dict[key])\n",
    "            elif slice == \"test\":\n",
    "                for key in test_losses_dict:\n",
    "                    test_losses_dict[key].append(loss_dict[key])\n",
    "            elif slice == \"val\":\n",
    "                for key in val_losses_dict:\n",
    "                    val_losses_dict[key].append(loss_dict[key])\n",
    "    # Convert lists to numpy arrays\n",
    "    train_losses_dict = {key: np.array(value) for key, value in train_losses_dict.items()}\n",
    "    test_losses_dict = {key: np.array(value) for key, value in test_losses_dict.items()}\n",
    "    val_losses_dict = {key: np.array(value) for key, value in val_losses_dict.items()}\n",
    "\n",
    "    return train_losses_dict, test_losses_dict, val_losses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_dicts_from_path(exp_path):\n",
    "    train_loss_dict = load_json_dict(os.path.join(exp_path, f\"train_loss\"))\n",
    "    test_loss_dict = load_json_dict(os.path.join(exp_path, f\"test_loss\"))\n",
    "    val_loss_dict = load_json_dict(os.path.join(exp_path, f\"val_loss\"))\n",
    "\n",
    "    return train_loss_dict, test_loss_dict, val_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_std_test_loss_from_path(exp_path, key):\n",
    "    test_loss_dict = load_pickle(os.path.join(exp_path, f\"test_loss_stats\"))\n",
    "    test_loss = test_loss_dict[key]\n",
    "    test_loss_avg = test_loss[\"avg\"]\n",
    "    test_loss_std = test_loss[\"std\"]\n",
    "    return test_loss_avg,test_loss_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the onset position as the distance between the the cluster centers\n",
    "# This info is stored inside onset_dict_list{0,50,...}, which contain the recorded cluster position\n",
    "# for all the samples in the test set\n",
    "def calculate_average_distance(records):\n",
    "    if not records:\n",
    "        return 0\n",
    "    total_distance = sum(item['avg_distance_clusters'] for item in records if 'avg_distance_clusters' in item)\n",
    "    return total_distance / len(records)\n",
    "\n",
    "def calculate_standard_deviation(records):\n",
    "    if not records:\n",
    "        return 0\n",
    "    avg_distance = calculate_average_distance(records)\n",
    "    variance = sum((item['avg_distance_clusters'] - avg_distance) ** 2 for item in records if 'avg_distance_clusters' in item) / len(records)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "def get_onset_position_errors(exp_path, removed_leads_range):\n",
    "    # removed_leads_range = range(0, 250, 50)\n",
    "    \n",
    "    onset_errors_dict = {\n",
    "        \"avg_d_min\": [],\n",
    "        \"std_d_min\": [],   \n",
    "    }\n",
    "\n",
    "    for num_removed_leads in removed_leads_range:\n",
    "        onset_dict_list = load_pickle(os.path.join(exp_path, f\"onsets_dict_list_{num_removed_leads}\"))\n",
    "        avg_onset_dist = calculate_average_distance(onset_dict_list)\n",
    "        std_dev_onset_dist = calculate_standard_deviation(onset_dict_list)\n",
    "        onset_errors_dict[\"avg_d_min\"].append(avg_onset_dist)\n",
    "        onset_errors_dict[\"std_d_min\"].append(std_dev_onset_dist)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    onset_errors_dict = {key: np.array(value) for key, value in onset_errors_dict.items()}\n",
    "\n",
    "    return onset_errors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This computes the onset position as the distance between the closest surface point to the cluster centers\n",
    "# # This info is stored inside onset_d_dict_{0,50,...}\n",
    "# def get_onset_position_errors(exp_path, removed_leads_range):\n",
    "#     # removed_leads_range = range(0, 250, 50)\n",
    "    \n",
    "#     onset_errors_dict = {\n",
    "#         \"avg_d_min\": [],\n",
    "#         \"std_d_min\": [],    \n",
    "#     }\n",
    "\n",
    "#     for num_removed_leads in removed_leads_range:\n",
    "#         loss_dict = load_json_dict(os.path.join(exp_path, f\"onset_d_dict_{num_removed_leads}\"))\n",
    "#         for key in onset_errors_dict:\n",
    "#             onset_errors_dict[key].append(loss_dict[key])\n",
    "\n",
    "#     # Convert lists to numpy arrays\n",
    "#     onset_errors_dict = {key: np.array(value) for key, value in onset_errors_dict.items()}\n",
    "\n",
    "#     return onset_errors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one example loss vs removed leads for latent-lead \n",
    "removed_leads_range = np.arange(0,250,20)\n",
    "example_path_latent_lead = \"/latent-lead/example/path\" # Change it to your example\n",
    "_, test_losses_dict_latent_lead, val_losses_dict_latent_lead = get_latent_lead_losses_with_removed_leads(example_path_latent_lead)\n",
    "test_acti_loss_array_latent_lead = test_losses_dict_latent_lead[\"acti_loss\"]\n",
    "# Get example test loss from vae_inria\n",
    "example_path_inria_vae = \"/vae_results/example/path\" \n",
    "_, test_losses_dict_inria_vae, _ = get_loss_dicts_from_path(example_path_inria_vae)\n",
    "max_acti_loss_vae_inria = test_losses_dict_inria_vae[\"rec_loss\"]\n",
    "# Plot test reconstruction loss for cardiac activation map\n",
    "plt.figure()\n",
    "plt.plot(removed_leads_range, test_acti_loss_array_latent_lead, color = \"blue\", label=\"Our model\")\n",
    "plt.axhline(y=max_acti_loss_vae_inria, color='red', linestyle='--', linewidth=1, label=\"Baseline\")\n",
    "# plt.vlines(x=134, ymin=np.min(test_acti_loss_array_latent_lead)-0.0002, ymax=max_acti_loss_vae_inria, colors='red', linestyles='--')\n",
    "plt.vlines(x=134, ymin=0.0, ymax=max_acti_loss_vae_inria, colors='red', linestyles='--')\n",
    "plt.xlim([0,250])\n",
    "plt.ylim([np.min(test_acti_loss_array_latent_lead)-0.0002, 0.0350])\n",
    "plt.ylim([0.0, 0.0350])\n",
    "plt.gca().tick_params(axis='x', which='both', direction='inout', length=6)\n",
    "plt.gca().set_xticks(list(plt.gca().get_xticks()) + [134])\n",
    "plt.xlabel(\"Number of removed leads\", fontsize=labelssize)\n",
    "plt.ylabel(\"Activation Loss\", fontsize=labelssize)\n",
    "plt.xticks(fontsize=ticksize)\n",
    "plt.yticks(fontsize=ticksize)\n",
    "plt.legend(fontsize=legendsize)\n",
    "save_png_eps_figure(\"acti_loss_vs_removed_leads_and_baseline_start_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_lead_result_dict_list = []\n",
    "for conf in latent_lead_completed_conf_files:\n",
    "    exp_path = conf[\"path\"]\n",
    "    lr = conf[\"optimizer\"][\"learning_rate\"]\n",
    "    reconstruction_signal_weight = conf[\"model\"][\"reconstruction_signal_weight\"]\n",
    "    alpha_loss_weight = conf[\"model\"][\"alpha_loss_weight\"]\n",
    "    md_name = conf[\"model\"][\"name\"]\n",
    "    ds_name = conf[\"dataset\"][\"name\"]\n",
    "    seed = conf[\"seed\"]\n",
    "    # Get training, validation, test loss values for multiple dropped leads\n",
    "    train_losses_dict, test_losses_dict, val_losses_dict = get_latent_lead_losses_with_removed_leads(exp_path)\n",
    "    # Get final training, validation, test loss values \n",
    "    train_loss, test_loss, val_loss  = get_loss_dicts_from_path(exp_path)\n",
    "    # Get statistics of test loss activation error\n",
    "    test_acti_loss_avg, test_acti_loss_std = get_avg_std_test_loss_from_path(exp_path,\"acti_loss\")\n",
    "    test_mae_acti_loss_avg, test_mae_acti_loss_std = get_avg_std_test_loss_from_path(exp_path,\"mae_acti_loss\")\n",
    "    # Get onset position errors for multiple dropped leads \n",
    "    onset_errors_dict = get_onset_position_errors(exp_path, removed_leads_range=[0,50,100,150,200])\n",
    "    # Get model size\n",
    "    model_size_dict = load_json_dict(os.path.join(exp_path,\"model_size\"))\n",
    "    model_size_mb = model_size_dict[\"model_size_mb\"] \n",
    "    num_parameters = model_size_dict[\"num_parameters\"] \n",
    "    gnn_num_parameters = model_size_dict[\"gnn\"][\"num_parameters\"] \n",
    "    fmmhead_num_parameters = model_size_dict[\"fmm_head\"][\"num_parameters\"] \n",
    "    encoder_num_parameters = model_size_dict[\"encoder\"][\"num_parameters\"] \n",
    "    decoder_num_parameters = model_size_dict[\"decoder\"][\"num_parameters\"] \n",
    "    # Get train time\n",
    "    train_time_dict = load_json_dict(os.path.join(exp_path,\"train_time\"))\n",
    "    train_time = train_time_dict[\"train_time\"]\n",
    "    avg_epoch_train_time = np.average(train_time_dict[\"train_epochs_time\"])\n",
    "    num_epochs = len(train_time_dict[\"train_epochs_time\"])\n",
    "    dict_to_add = {\"Dataset\": ds_name, \"Model\": md_name, \"Learning Rate\":lr, \"Seed\":seed,\n",
    "                   \"reconstruction_signal_weight\": reconstruction_signal_weight,\n",
    "                   \"alpha_loss_weight\": alpha_loss_weight,\n",
    "                   \"model_num_parameters\":num_parameters,\n",
    "                   \"model_size_mb\": model_size_mb,\n",
    "                   \"gnn_num_parameters\": gnn_num_parameters,\n",
    "                   \"fmmhead_num_parameters\": fmmhead_num_parameters,\n",
    "                   \"encoder_num_parameters\": encoder_num_parameters,\n",
    "                   \"decoder_num_parameters\": decoder_num_parameters,\n",
    "                   \"train_time\":train_time,\n",
    "                   \"avg_epoch_train_time\": avg_epoch_train_time,\n",
    "                   \"num_epochs\":num_epochs,\n",
    "                    #  Train loss with removed leads\n",
    "                    \"train_loss_removed_leads\": train_losses_dict[\"loss\"],\n",
    "                    \"train_acti_loss_removed_leads\": train_losses_dict[\"acti_loss\"],\n",
    "                    \"train_signal_loss_removed_leads\": train_losses_dict[\"signal_loss\"],\n",
    "                    \"train_mae_acti_loss_removed_leads\": train_losses_dict[\"mae_acti_loss\"],\n",
    "                    \"train_loss_best_final_value\": train_loss[\"loss\"],\n",
    "                    \"train_acti_loss_best_final_value\": train_loss[\"acti_loss\"],\n",
    "                    \"train_signal_loss_best_final_value\": train_loss[\"signal_loss\"],\n",
    "                    \"train_mae_acti_loss_best_final_value\": train_loss[\"mae_acti_loss\"],\n",
    "\n",
    "                    #  Val loss with removed leads\n",
    "                    \"val_loss_removed_leads\": val_losses_dict[\"loss\"],\n",
    "                    \"val_acti_loss_removed_leads\": val_losses_dict[\"acti_loss\"],\n",
    "                    \"val_signal_loss_removed_leads\": val_losses_dict[\"signal_loss\"],\n",
    "                    \"val_mae_acti_loss_removed_leads\": val_losses_dict[\"mae_acti_loss\"],\n",
    "                    \"val_loss_best_final_value\": val_loss[\"loss\"],\n",
    "                    \"val_acti_loss_best_final_value\": val_loss[\"acti_loss\"],\n",
    "                    \"val_signal_loss_best_final_value\": val_loss[\"signal_loss\"],\n",
    "                    \"val_mae_acti_loss_best_final_value\": val_loss[\"mae_acti_loss\"],\n",
    "\n",
    "                    #  Test loss with removed leads\n",
    "                    \"test_loss_removed_leads\": test_losses_dict[\"loss\"],\n",
    "                    \"test_acti_loss_removed_leads\": test_losses_dict[\"acti_loss\"],\n",
    "                    \"test_signal_loss_removed_leads\": test_losses_dict[\"signal_loss\"],\n",
    "                    \"test_mae_acti_loss_removed_leads\": test_losses_dict[\"mae_acti_loss\"],\n",
    "                    \"test_loss_best_final_value\": test_loss[\"loss\"],\n",
    "                    \"test_acti_loss_best_final_value\": test_loss[\"acti_loss\"],\n",
    "                    \"test_signal_loss_best_final_value\": test_loss[\"signal_loss\"],\n",
    "                    \"test_mae_acti_loss_best_final_value\": test_loss[\"mae_acti_loss\"],\n",
    "\n",
    "                    # Test loss statistics\n",
    "                    \"test_acti_loss_avg\": test_acti_loss_avg,\n",
    "                    \"test_acti_loss_std\": test_acti_loss_std,\n",
    "                    \"test_mae_acti_loss_avg\":test_mae_acti_loss_avg,\n",
    "                    \"test_mae_acti_loss_std\":test_mae_acti_loss_std,\n",
    "                    \n",
    "                    # Spatial distances\n",
    "                    \"onset_avg_d_min_removed_leads\": onset_errors_dict[\"avg_d_min\"],\n",
    "                    \"onset_std_d_min_removed_leads\": onset_errors_dict[\"std_d_min\"],\n",
    "                    \"onset_avg_d_min_best_final_value\": onset_errors_dict[\"avg_d_min\"][0],\n",
    "                    \"onset_std_d_min_best_final_value\": onset_errors_dict[\"std_d_min\"][0],\n",
    "    }\n",
    "\n",
    "    latent_lead_result_dict_list.append(dict_to_add)\n",
    "# latent_lead_result_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_lead_df = pd.DataFrame(latent_lead_result_dict_list)\n",
    "latent_lead_df_test_loss = latent_lead_df[[\"Model\",\"Learning Rate\",\n",
    "                                           \"reconstruction_signal_weight\",\n",
    "                                           \"alpha_loss_weight\",\n",
    "                                           \"test_acti_loss_avg\",\n",
    "                                           \"test_acti_loss_std\",\n",
    "                                           \"test_mae_acti_loss_avg\",\n",
    "                                           \"test_mae_acti_loss_std\",\n",
    "                                           \"onset_avg_d_min_best_final_value\",\n",
    "                                           \"onset_std_d_min_best_final_value\",\n",
    "                                           \"Seed\"]]\n",
    "# Groupy hyperparameters and take only first experiment if there are multiple ones\n",
    "grouped_latent_lead_df_test_loss = latent_lead_df_test_loss.groupby([\"Learning Rate\", \"reconstruction_signal_weight\", \"alpha_loss_weight\"]).first().reset_index()\n",
    "\n",
    "best_model = grouped_latent_lead_df_test_loss.loc[grouped_latent_lead_df_test_loss[\"test_acti_loss_avg\"].idxmin()]\n",
    "best_learning_rate = best_model[\"Learning Rate\"]\n",
    "best_reconstruction_signal_weight = best_model[\"reconstruction_signal_weight\"]\n",
    "\n",
    "print(best_learning_rate, best_reconstruction_signal_weight)\n",
    "grouped_latent_lead_df_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_inria_result_dict_list = []\n",
    "for conf in inria_vae_completed_conf_files:\n",
    "    exp_path = conf[\"path\"]\n",
    "    lr = conf[\"optimizer\"][\"learning_rate\"]\n",
    "    md_name = conf[\"model\"][\"name\"]\n",
    "    ds_name = conf[\"dataset\"][\"name\"]\n",
    "    seed = conf[\"seed\"]\n",
    "    \n",
    "    # Get training, validation, test loss values\n",
    "    train_losses_dict, test_losses_dict, val_losses_dict = get_loss_dicts_from_path(exp_path)\n",
    "\n",
    "    # Get statistics of test loss activation error\n",
    "    test_acti_loss_avg, test_acti_loss_std = get_avg_std_test_loss_from_path(exp_path,\"rec_loss\")\n",
    "    test_mae_acti_loss_avg, test_mae_acti_loss_std = get_avg_std_test_loss_from_path(exp_path,\"rec_loss_mae\")\n",
    "\n",
    "    # Get onset position errors for multiple dropped leads \n",
    "    onset_errors_dict = get_onset_position_errors(exp_path, removed_leads_range=[0])\n",
    "    \n",
    "    # Get model size\n",
    "    model_size_dict = load_json_dict(os.path.join(exp_path, \"model_size\"))\n",
    "    model_size_mb = model_size_dict[\"model_size_mb\"]\n",
    "    num_parameters = model_size_dict[\"num_parameters\"]\n",
    "    \n",
    "    # Get train time\n",
    "    train_time_dict = load_json_dict(os.path.join(exp_path, \"train_time\"))\n",
    "    train_time = train_time_dict[\"train_time\"]\n",
    "    avg_epoch_train_time = np.average(train_time_dict[\"train_epochs_time\"])\n",
    "    num_epochs = len(train_time_dict[\"train_epochs_time\"])\n",
    "\n",
    "    dict_to_add = {\n",
    "        \"Dataset\": ds_name,\n",
    "        \"Model\": md_name,\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Seed\": seed,\n",
    "\n",
    "        \"model_num_parameters\": num_parameters,\n",
    "        \"model_size_mb\": model_size_mb,\n",
    "        \"train_time\": train_time,\n",
    "        \"avg_epoch_train_time\": avg_epoch_train_time,\n",
    "        \"num_epochs\": num_epochs,\n",
    "\n",
    "        # Train loss \n",
    "        \"train_loss\": train_losses_dict[\"loss\"],\n",
    "        \"train_rec_loss\": train_losses_dict[\"rec_loss\"],\n",
    "        \"train_rec_loss_gaus\": train_losses_dict[\"rec_loss_gaus\"],\n",
    "        \"train_kl_loss\": train_losses_dict[\"kl_loss\"],\n",
    "        \"train_rec_loss_mae\": train_losses_dict[\"rec_loss_mae\"],\n",
    "\n",
    "        # Val loss \n",
    "        \"val_loss\": val_losses_dict[\"loss\"],\n",
    "        \"val_rec_loss\": val_losses_dict[\"rec_loss\"],\n",
    "        \"val_rec_loss_gaus\": val_losses_dict[\"rec_loss_gaus\"],\n",
    "        \"val_kl_loss\": val_losses_dict[\"kl_loss\"],\n",
    "        \"val_rec_loss_mae\": val_losses_dict[\"rec_loss_mae\"],\n",
    "\n",
    "        # Test loss \n",
    "        \"test_loss\": test_losses_dict[\"loss\"],\n",
    "        \"test_rec_loss\": test_losses_dict[\"rec_loss\"],\n",
    "        \"test_rec_loss_gaus\": test_losses_dict[\"rec_loss_gaus\"],\n",
    "        \"test_kl_loss\": test_losses_dict[\"kl_loss\"],\n",
    "        \"test_rec_loss_mae\": test_losses_dict[\"rec_loss_mae\"],\n",
    "\n",
    "        # Test loss statistics\n",
    "        \"test_acti_loss_avg\": test_acti_loss_avg,\n",
    "        \"test_acti_loss_std\": test_acti_loss_std,\n",
    "        \"test_mae_acti_loss_avg\":test_mae_acti_loss_avg,\n",
    "        \"test_mae_acti_loss_std\":test_mae_acti_loss_std,\n",
    "\n",
    "        # Spatial distances\n",
    "        \"onset_avg_d_min_best_final_value\": onset_errors_dict[\"avg_d_min\"][0],\n",
    "        \"onset_std_d_min_best_final_value\": onset_errors_dict[\"std_d_min\"][0],\n",
    "    }\n",
    "\n",
    "    vae_inria_result_dict_list.append(dict_to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria_vae_df = pd.DataFrame(vae_inria_result_dict_list)\n",
    "inria_vae_df_test_loss = inria_vae_df[[\"Model\",\"Learning Rate\",\n",
    "                                       \"test_rec_loss\",\n",
    "                                       \"test_rec_loss_mae\",\n",
    "                                       \"test_acti_loss_avg\",\n",
    "                                       \"test_acti_loss_std\",\n",
    "                                       \"test_mae_acti_loss_avg\",\n",
    "                                       \"test_mae_acti_loss_std\",\n",
    "                                       \"onset_avg_d_min_best_final_value\",\n",
    "                                       \"onset_std_d_min_best_final_value\",\n",
    "                                       \"Seed\"]]\n",
    "grouped_inria_vae_df_test_loss = inria_vae_df_test_loss.groupby([\"Learning Rate\"]).first().reset_index()\n",
    "\n",
    "best_model = grouped_inria_vae_df_test_loss.loc[grouped_inria_vae_df_test_loss[\"test_rec_loss\"].idxmin()]\n",
    "best_learning_rate = best_model[\"Learning Rate\"]\n",
    "\n",
    "print(best_learning_rate)\n",
    "grouped_inria_vae_df_test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecgi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
