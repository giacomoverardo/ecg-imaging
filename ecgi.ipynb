{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate, call\n",
    "import logging \n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import tqdm\n",
    "import SimpleITK as sitk\n",
    "import pyvista as pv\n",
    "from src.utils.general import set_ml_seeds, create_folder, save_dict_to_binary, load_dict_from_binary\n",
    "from src.utils.metrics import compute_losses_for_dataset, compute_average_and_std\n",
    "from src.plot.activation_map import (   \n",
    "                                    save_acti_map_fig,                         \n",
    "                                    create_tetra_links_and_reference_images,\n",
    "                                    find_min_max_activation_point,\n",
    "                                    find_min_activation_point_k_means,\n",
    "                                    to_tetrahedral)\n",
    "from src.dataset.inria import ( \n",
    "                        load_inria_patient_electrodes_mesh, \n",
    "                        get_inria_small_mesh_path, \n",
    "                        get_inria_sample_path, \n",
    "                        get_geometry_info_per_patient_dict, \n",
    "                        get_masks_per_patient_dict)\n",
    "from src.utils.nn import get_parameters_count_from_model\n",
    "from src.utils.math import compute_closest_points_combination\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "logger = logging.getLogger(__name__)\n",
    "initialize('conf',version_base=\"1.3\")\n",
    "# Update overrides to run vae-inria or latent-lead. Parameters can be found in conf directory and subdirectories\n",
    "cfg = compose(config_name='ecgi-local.yaml',return_hydra_config=True,\n",
    "              overrides=[\"hydra.verbose=true\",\"dataset=inria\",\n",
    "                         \"model=vae-inria\",                  \n",
    "                        #  \"model=latent-lead\",\n",
    "                        #  \"model/gnn=gat_model\",\n",
    "                         \"batch_size=32\", \n",
    "                        \"optimizer=adam\",\n",
    "                        \"optimizer.learning_rate=0.0001\", \n",
    "                         \"train.num_epochs=0\",\n",
    "                         \"save_plots=True\",\n",
    "                         ]) \n",
    "set_ml_seeds(cfg.seed)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration file\n",
    "create_folder(cfg.model.tb_output_dir)\n",
    "filename = os.path.join(cfg.model.tb_output_dir,\"conf.yaml\")\n",
    "with open(filename,\"w\") as fp:\n",
    "    OmegaConf.save(config=cfg, f=fp)\n",
    "#Image saving function\n",
    "def save_png_eps_figure(filename):\n",
    "    if(cfg.save_plots):\n",
    "        full_filename = os.path.join(cfg.model.tb_output_dir,filename)\n",
    "        plt.savefig(full_filename+\".png\")\n",
    "        # plt.savefig(full_filename+\".eps\")\n",
    "        plt.savefig(full_filename+\".pdf\")\n",
    "# Dict saving function\n",
    "def save_dict(in_dict, filename):\n",
    "    if(cfg.save_plots):\n",
    "        full_filename = os.path.join(cfg.model.tb_output_dir, filename)\n",
    "        with open(full_filename, \"w\") as f:\n",
    "            json.dump(in_dict, f)\n",
    "def load_dict(filename):\n",
    "    full_filename = os.path.join(cfg.model.tb_output_dir, filename)\n",
    "    with open(full_filename, \"r\") as f:\n",
    "        return json.load(f)\n",
    "#Numpy saving function\n",
    "def save_np(np_array, filename):\n",
    "    if(cfg.save_plots):\n",
    "        full_filename = os.path.join(cfg.model.tb_output_dir,filename)\n",
    "        np.save(full_filename, np_array)\n",
    "# Pickle saving function\n",
    "def save_pickle(in_pickle, filename):   \n",
    "    full_filename = os.path.join(cfg.model.tb_output_dir,filename)   \n",
    "    with open(full_filename, 'wb') as file:\n",
    "        pickle.dump(in_pickle, file)\n",
    "def load_pickle(filename):\n",
    "    full_filename = os.path.join(cfg.model.tb_output_dir, filename)\n",
    "    with open(full_filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "# Plot settings\n",
    "labelssize = 11\n",
    "ticksize = 12\n",
    "legendsize = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_ds, val_ds, test_ds = call(cfg.dataset.load_function)\n",
    "train_ds_len = len(train_ds)\n",
    "val_ds_len = len(val_ds)\n",
    "test_ds_len = len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = instantiate(cfg.model)\n",
    "#Instantiate optimizer and compile the model\n",
    "opt = instantiate(cfg.optimizer)\n",
    "model.compile(optimizer=opt)\n",
    "# Instantiate callbacks\n",
    "callbacks = [instantiate(cfg.callbacks[c]) for c in cfg.callbacks]\n",
    "training_time_cb_index = list(cfg.callbacks.keys()).index(\"training-time\")\n",
    "outs = model({\"acti_map\":tf.zeros((cfg.batch_size,120,120,120)),\n",
    "            \"conduct\":tf.zeros((cfg.batch_size,120,120,120)),\n",
    "            \"mask\":tf.zeros((cfg.batch_size,120,120,120)),\n",
    "            \"signal\":tf.zeros((cfg.batch_size,260,450)),\n",
    "            \"adj_matrix\":tf.zeros((cfg.batch_size,260,260)),\n",
    "            \"patient_id\":tf.zeros((cfg.batch_size,)),\n",
    "            \"peaks_location\":tf.zeros((cfg.batch_size,2))},\n",
    "            training=False)\n",
    "# Test if evaluate and predict work\n",
    "# model.evaluate(train_ds.batch(cfg.batch_size).take(1))\n",
    "# model.predict(train_ds.batch(cfg.batch_size).take(1))\n",
    "# model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and collect history and other results\n",
    "start_train_time = time.time()\n",
    "history = model.fit(train_ds.batch(cfg.batch_size,drop_remainder=True).take(1).prefetch(tf.data.AUTOTUNE), epochs=cfg.train.num_epochs, \n",
    "                    validation_data=val_ds.batch(cfg.batch_size,drop_remainder=True).take(1).prefetch(tf.data.AUTOTUNE), callbacks=callbacks)\n",
    "end_train_time = time.time()\n",
    "train_time = end_train_time-start_train_time\n",
    "train_epoch_training_times = callbacks[training_time_cb_index].epoch_times\n",
    "save_dict({\"train_time\":train_time,\n",
    "           \"train_epochs_time\":train_epoch_training_times,\n",
    "           },\"train_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(cfg.model.tb_output_dir,\"checkpoint\") #Restore best model \n",
    "def reload_best_model_with_leads(num_removed_leads:int=0):\n",
    "    # Load best model from checkpoint and sets the number of employed leads if specified\n",
    "    # Load best model from checkpoint\n",
    "    model = instantiate(cfg.model)\n",
    "    model.load_weights(checkpoint_path).expect_partial()\n",
    "    # Set number of leads if specified\n",
    "    if(num_removed_leads>0):\n",
    "        model.num_kept_leads = model.num_leads - num_removed_leads\n",
    "    # Build the model by calling it on an example data\n",
    "    # model.predict(train_ds.batch(cfg.batch_size).take(1))\n",
    "    model({\"acti_map\":tf.zeros((cfg.batch_size,120,120,120)),\n",
    "                \"conduct\":tf.zeros((cfg.batch_size,120,120,120)),\n",
    "                \"mask\":tf.zeros((cfg.batch_size,120,120,120)),\n",
    "                \"signal\":tf.random.uniform((cfg.batch_size,260,450)),\n",
    "                \"adj_matrix\":tf.zeros((cfg.batch_size,260,260)),\n",
    "                \"peaks_location\":tf.zeros((cfg.batch_size,2)),\n",
    "                \"patient_id\":tf.zeros((cfg.batch_size,))},\n",
    "                training=False)\n",
    "    # Compile the model\n",
    "    opt = instantiate(cfg.optimizer)\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "model = reload_best_model_with_leads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(cfg.save_plots):\n",
    "    if(cfg.model.name in [\"latent-lead\"]):\n",
    "        if(cfg.model.gnn.name!=\"uniform\"):\n",
    "            # Evaluate model for different number of employed leads\n",
    "            removed_leads_range = range(0, 250, 150) #take(1)\n",
    "            for num_removed_leads in removed_leads_range:\n",
    "                model = reload_best_model_with_leads(num_removed_leads)\n",
    "                # Compute losses of final model on training and test sets\n",
    "                train_loss_dict = model.evaluate(train_ds.batch(cfg.batch_size, drop_remainder=True).take(1),return_dict=True)\n",
    "                filename = os.path.join(cfg.model.tb_output_dir,f\"train_loss_{num_removed_leads}\")\n",
    "                save_dict(train_loss_dict,filename)\n",
    "                val_loss_dict = model.evaluate(val_ds.batch(cfg.batch_size, drop_remainder=True).take(1),return_dict=True)\n",
    "                filename = os.path.join(cfg.model.tb_output_dir,f\"val_loss_{num_removed_leads}\")\n",
    "                save_dict(val_loss_dict,filename)\n",
    "                test_loss_dict = model.evaluate(test_ds.batch(cfg.batch_size, drop_remainder=True).take(1),return_dict=True)\n",
    "                filename = os.path.join(cfg.model.tb_output_dir,f\"test_loss_{num_removed_leads}\")\n",
    "                save_dict(test_loss_dict,filename)\n",
    "# Plot test losses from previous cell results (different number of input leads)\n",
    "if(cfg.save_plots):\n",
    "    if(cfg.model.name in [\"latent-lead\"]):\n",
    "        if(cfg.model.gnn.name!=\"uniform\"):\n",
    "            results_less_leads_dict = {key:np.zeros(len(removed_leads_range),dtype=float) for key in test_loss_dict.keys()}\n",
    "            results_less_leads_dict[\"num_removed_leads\"] = np.zeros(len(removed_leads_range))\n",
    "            for l_i, num_removed_leads in enumerate(removed_leads_range):\n",
    "                filename = os.path.join(cfg.model.tb_output_dir,f\"test_loss_{num_removed_leads}\")\n",
    "                test_loss_dict = load_dict(filename)\n",
    "                for key in test_loss_dict.keys():\n",
    "                    key_array = results_less_leads_dict[key]\n",
    "                    key_array[l_i] = test_loss_dict[key]\n",
    "                    results_less_leads_dict[key] =  key_array\n",
    "                num_removed_leads_array = results_less_leads_dict[\"num_removed_leads\"]\n",
    "                num_removed_leads_array[l_i] = num_removed_leads\n",
    "                results_less_leads_dict[\"num_removed_leads\"] =  num_removed_leads_array\n",
    "            map_plot_dict = {'loss': \"Loss\",\n",
    "            'acti_loss': \"Activation Loss\",\n",
    "            'signal_loss': \"Signal Loss\",\n",
    "            'mae_acti_loss': \"MAE Activation Loss\",\n",
    "            'alpha_loss': \"FMM localization error\"}\n",
    "            for key in test_loss_dict.keys():\n",
    "                plt.figure()\n",
    "                plt.plot(results_less_leads_dict[\"num_removed_leads\"],results_less_leads_dict[key])\n",
    "                plt.xlabel(\"Number of removed leads\", fontsize=labelssize)\n",
    "                plt.ylabel(map_plot_dict[key], fontsize=labelssize)\n",
    "                plt.xticks(fontsize=ticksize)\n",
    "                plt.yticks(fontsize=ticksize)\n",
    "                plt.legend(fontsize=legendsize)\n",
    "                save_png_eps_figure(f\"{key}_vs_removed_leads\")\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload full model\n",
    "model = reload_best_model_with_leads()\n",
    "# Compute and save model size\n",
    "model_parameters_count_dict = get_parameters_count_from_model(model)\n",
    "model_size_bytes = os.path.getsize(checkpoint_path+\".data-00000-of-00001\")\n",
    "model_size_mb = model_size_bytes / (1024 * 1024)\n",
    "model_size_dict = {\"num_trainable\": model_parameters_count_dict[\"num_trainable\"],\n",
    "                    \"num_non_trainable\": model_parameters_count_dict[\"num_non_trainable\"],\n",
    "                    \"num_parameters\": model_parameters_count_dict[\"num_parameters\"],\n",
    "                    \"model_size_bytes\": model_size_bytes,\n",
    "                    \"model_size_mb\": model_size_mb,\n",
    "                    }\n",
    "if(cfg.model.name in [\"latent-lead\"]):\n",
    "    for submodel,submodel_name in zip([model.gnn, model.encoder, model.decoder, model.fmm_head],\n",
    "                                      [\"gnn\", \"encoder\", \"decoder\", \"fmm_head\"]):\n",
    "        submodel_parameters_count_dict = get_parameters_count_from_model(submodel)\n",
    "        model_size_dict[submodel_name] = submodel_parameters_count_dict\n",
    "if(cfg.save_plots):\n",
    "    save_dict(model_size_dict,\"model_size\")\n",
    "print(model_size_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(cfg.save_plots):\n",
    "    if(cfg.model.name in [\"latent-lead\"]):\n",
    "        from src.plot.fmm import plot_fmm_wave_from_coefficients\n",
    "        # Plot FMM waves coefficients and the lead weights from the gnn\n",
    "        num_samples_to_plot = 5\n",
    "        lead_to_plot = 100\n",
    "        for ds,ds_name in zip([test_ds,train_ds],[\"test\",\"train\"]):\n",
    "            for sample in ds.shuffle(100,seed=cfg.seed).take(cfg.batch_size).batch(cfg.batch_size):\n",
    "                predict_dict = model.predict(sample)\n",
    "                patient_ids = sample[\"patient_id\"].numpy()\n",
    "                onsets = sample[\"onset\"].numpy()\n",
    "                file_ids = sample[\"file_id\"].numpy()\n",
    "                for i in range(num_samples_to_plot):\n",
    "                    patient_id = patient_ids[i].decode('utf-8')\n",
    "                    onset = onsets[i].decode('utf-8')\n",
    "                    file_id = file_ids[i].decode('utf-8')\n",
    "                    original = predict_dict[\"signal\"][i]\n",
    "                    # Extract FMM coefficients\n",
    "                    fmm_coeffs = predict_dict[\"predicted_fmm_coefficients\"][i]\n",
    "                    sample_len = cfg.dataset.sequence_length\n",
    "                    xaxis = np.arange(1,sample_len+1)/cfg.dataset.fs\n",
    "                    # Plot original ECG time series\n",
    "                    plt.figure()\n",
    "                    plt.plot(xaxis,np.squeeze(original[:,lead_to_plot]),label=\"Original\",color=\"b\")\n",
    "                    plt.xlabel(\"Time [s]\",fontsize=labelssize)\n",
    "                    plt.ylabel(\"ECG\",fontsize=labelssize)\n",
    "                    plt.xticks(fontsize=ticksize)\n",
    "                    plt.yticks(fontsize=ticksize)\n",
    "                    # Plot ECG data\n",
    "                    plot_fmm_wave_from_coefficients(fmm_coeff_array=fmm_coeffs,\n",
    "                                                    num_leads=cfg.dataset.num_electrodes,\n",
    "                                                    num_waves = cfg.model.num_waves,\n",
    "                                                    seq_len=cfg.dataset.sequence_length,\n",
    "                                                    fs=cfg.dataset.fs,\n",
    "                                                    lead=lead_to_plot,\n",
    "                                                    add_single_waves=True,\n",
    "                                                    label=\"Predicted\",\n",
    "                                                    wave_label_type=\"number\")\n",
    "                    plt.legend(fontsize=legendsize)\n",
    "                    save_png_eps_figure(f\"fmm_reconstruction_{ds_name}_{patient_id}_{onset}_{file_id}_lead_{lead_to_plot}\")\n",
    "                    plt.close()\n",
    "                    # Plot the lead weights from the gnn\n",
    "                    # Get electrodes mesh from data folder for the correct patient\n",
    "                    electrode_mesh = load_inria_patient_electrodes_mesh(cfg.datapath,patient_id)\n",
    "                    # Get output weights from the gnn\n",
    "                    lead_weight_sample = predict_dict[\"lead_weights\"][i]\n",
    "                    # Do the scatter plot of the weights\n",
    "                    fig = plt.figure()\n",
    "                    ax = fig.add_subplot(projection='3d')\n",
    "                    sc = ax.scatter(electrode_mesh[:, 0], electrode_mesh[:, 1], electrode_mesh[:, 2], c=lead_weight_sample)\n",
    "                    ax.set_xlabel(\"x\", fontsize=labelssize)\n",
    "                    ax.set_ylabel(\"y\", fontsize=labelssize)\n",
    "                    ax.set_zlabel(\"z\", fontsize=labelssize)\n",
    "                    ax.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "                    ax.tick_params(axis='z', which='major', labelsize=ticksize)\n",
    "                    ax.view_init(elev=20, azim=145)\n",
    "                    fig.colorbar(sc)\n",
    "                    save_png_eps_figure(f\"gnn_lead_weights_{ds_name}_{patient_id}_{onset}_{file_id}\")\n",
    "                    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def history_plot_fun(history, file_name):\n",
    "    plt.figure()\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    if(cfg.model.name==\"vae-inria\"):\n",
    "        plot_list = [['loss','val_loss'],['rec_loss','val_rec_loss'], ['rec_loss_gaus','val_rec_loss_gaus'],['kl_loss','val_kl_loss']]\n",
    "    elif(cfg.model.name==\"latent-lead\"):\n",
    "        plot_list = [['loss','val_loss'],['acti_loss','val_acti_loss'],['signal_loss','val_signal_loss'], ['alpha_loss','val_alpha_loss']]\n",
    "    plot_list.append(['lr'])\n",
    "    for i,metric_to_plot_list in enumerate(plot_list):\n",
    "        ax = plt.subplot(len(plot_list),1,i+1)\n",
    "        for metric_to_plot in metric_to_plot_list:\n",
    "            to_plot = history.history[metric_to_plot]\n",
    "            ax.plot(to_plot,label=metric_to_plot)\n",
    "        plt.legend()\n",
    "        ax.set_title(metric_to_plot_list[0].capitalize())\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if(i!=len(plot_list)-1):\n",
    "            ax.tick_params(\n",
    "                axis='x',          # changes apply to the x-axis\n",
    "                which='both',      # both major and minor ticks are affected\n",
    "                bottom=False,      # ticks along the bottom edge are off\n",
    "                top=False,         # ticks along the top edge are off\n",
    "                labelbottom=False)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    if(cfg.save_plots):\n",
    "        filename = os.path.join(cfg.model.tb_output_dir, f\"{file_name}.json\")\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(history.history, f)\n",
    "        save_png_eps_figure(f\"{file_name}\")\n",
    "if(history is not None):\n",
    "    history_plot_fun(history=history, file_name=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(cfg.save_plots):\n",
    "    if(cfg.dataset.name==\"inria\"):\n",
    "        num_samples_to_plot = 5\n",
    "        # for ds,slice in zip([test_ds,train_ds],[\"test\",\"train\"]):\n",
    "        for ds,slice in zip([test_ds],[\"test\"]):\n",
    "            # for sample in ds.shuffle(100,seed=cfg.seed).take(cfg.batch_size).batch(cfg.batch_size):\n",
    "            for sample in ds.take(cfg.batch_size).batch(cfg.batch_size):\n",
    "                predict_dict = model.predict(sample)\n",
    "                patient_ids = sample[\"patient_id\"].numpy()\n",
    "                onsets = sample[\"onset\"].numpy()\n",
    "                file_ids = sample[\"file_id\"].numpy()\n",
    "                # Collect groundtruth, predicted activation map and mask for the sample batch \n",
    "                ground_truth_acti_map_tens = tf.sparse.to_dense(predict_dict[\"acti_map\"])\n",
    "                predicted_acti_map_tens = predict_dict[\"predicted_acti_map\"]\n",
    "                mask_tens = tf.cast(tf.sparse.to_dense(predict_dict[\"mask\"]),tf.bool)\n",
    "                # Mask activation maps with the extracted mask\n",
    "                ground_truth_acti_map_tens_masked = tf.where(mask_tens, ground_truth_acti_map_tens, tf.zeros_like(ground_truth_acti_map_tens))\n",
    "                # ground_truth_acti_map_tens_masked = (\n",
    "                #     1 * tf.cast(mask_tens, ground_truth_acti_map_tens_masked.dtype) - ground_truth_acti_map_tens_masked\n",
    "                # )          \n",
    "                predicted_acti_map_tens_masked = tf.where(mask_tens, predicted_acti_map_tens, tf.zeros_like(predicted_acti_map_tens))\n",
    "                #For prediction, use 1.0 - predicted_acti_map to invert as in the loss function\n",
    "                predicted_acti_map_tens_masked = (\n",
    "                    1 * tf.cast(mask_tens, predicted_acti_map_tens_masked.dtype) - predicted_acti_map_tens_masked\n",
    "                )                         \n",
    "                for i in range(num_samples_to_plot):\n",
    "                    ground_truth_acti_map = ground_truth_acti_map_tens_masked.numpy()[i]\n",
    "                    predicted_acti_map = predicted_acti_map_tens_masked.numpy()[i]\n",
    "                    try:\n",
    "                        # Save the predicted activation map figure\n",
    "                        save_acti_map_fig(acti_map=predicted_acti_map, \n",
    "                                        data_path=cfg.datapath,\n",
    "                                        save_path=cfg.model.tb_output_dir,\n",
    "                                        slice=slice,\n",
    "                                        patient_id=patient_ids[i].decode('utf-8'),\n",
    "                                        onset=onsets[i].decode('utf-8'),\n",
    "                                        file_id=file_ids[i].decode('utf-8'),\n",
    "                                        add_text=slice+\"_pred\")\n",
    "                        # Save the real activation map figure\n",
    "                        save_acti_map_fig(acti_map=ground_truth_acti_map,\n",
    "                                        data_path=cfg.datapath,\n",
    "                                        save_path=cfg.model.tb_output_dir,\n",
    "                                        slice=slice,\n",
    "                                        patient_id=patient_ids[i].decode('utf-8'),\n",
    "                                        onset=onsets[i].decode('utf-8'),\n",
    "                                        file_id=file_ids[i].decode('utf-8'),\n",
    "                                        add_text=slice+\"_real\")\n",
    "                        # # Plot the abs difference of the predicted and real activation map\n",
    "                        # save_acti_map_fig(acti_map=np.abs(ground_truth_acti_map-predicted_acti_map),\n",
    "                        #                 data_path=cfg.datapath,\n",
    "                        #                 save_path=cfg.model.tb_output_dir,\n",
    "                        #                 slice=slice,\n",
    "                        #                 patient_id=patient_ids[i].decode('utf-8'),\n",
    "                        #                 onset=onsets[i].decode('utf-8'),\n",
    "                        #                 file_id=file_ids[i].decode('utf-8'),\n",
    "                        #                 add_text=slice+\"_difference\")   \n",
    "                    except:\n",
    "                        a=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_onset_position(in_dataset, mask_per_patient_dict, tetra_links_dict, mesh_dict, reference_images_dict, geometry_info_dict, transformer):\n",
    "    \"\"\"Computes onest position for predicted and real activation map and their distance \n",
    "\n",
    "    Args:\n",
    "        in_dataset (_type_): dataset\n",
    "        other_inputs: in order to have speed up, some structures are presaved and given as input\n",
    "    Outs:\n",
    "        a list of dictionaries containing patient_id, onset, file_id, real onset position, predicted onset position, their distance \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    results_list = []\n",
    "    for sample in tqdm.tqdm(in_dataset.batch(cfg.batch_size, drop_remainder=True)): #.take(1)\n",
    "        predict_dict = model.predict(sample,verbose=0)\n",
    "        patient_ids = sample[\"patient_id\"].numpy()\n",
    "        onsets = sample[\"onset\"].numpy()\n",
    "        file_ids = sample[\"file_id\"].numpy()\n",
    "\n",
    "\n",
    "        # Collect groundtruth, predicted activation map and mask for the sample batch \n",
    "        ground_truth_acti_map_tens = tf.sparse.to_dense(predict_dict[\"acti_map\"])\n",
    "        predicted_acti_map_tens = predict_dict[\"predicted_acti_map\"]\n",
    "        mask_tens = tf.cast(tf.sparse.to_dense(predict_dict[\"mask\"]),tf.bool)\n",
    "        # Mask activation maps with the extracted mask\n",
    "        ground_truth_acti_map_tens_masked = tf.where(mask_tens, ground_truth_acti_map_tens, tf.zeros_like(ground_truth_acti_map_tens))\n",
    "        predicted_acti_map_tens_masked = tf.where(mask_tens, predicted_acti_map_tens, tf.zeros_like(predicted_acti_map_tens))\n",
    "        #For prediction, use 1.0 - predicted_acti_map to invert as in the loss function\n",
    "        predicted_acti_map_tens_masked = (\n",
    "            1 * tf.cast(mask_tens, predicted_acti_map_tens_masked.dtype) - predicted_acti_map_tens_masked\n",
    "        )                         \n",
    "        for i in range(cfg.batch_size):\n",
    "            patient_id=patient_ids[i].decode('utf-8')\n",
    "            onset=onsets[i].decode('utf-8')\n",
    "            file_id=file_ids[i].decode('utf-8')\n",
    "            ground_truth_acti_map = ground_truth_acti_map_tens_masked.numpy()[i]\n",
    "            predicted_acti_map = predicted_acti_map_tens_masked.numpy()[i]\n",
    "            num_onsets = {\"one_init_rv\":1,\"two_init_lv\":2,\"three_init\":3}.get(onset)\n",
    "            onsets_positions, cluster_centers = find_min_activation_point_k_means(acti_map=ground_truth_acti_map,\n",
    "                                                num_onsets = num_onsets,\n",
    "                                                acti_mask=mask_per_patient_dict[patient_id],\n",
    "                                                tetra_links=tetra_links_dict[patient_id], \n",
    "                                                tetra_polydata=mesh_dict[patient_id],\n",
    "                                                ref_img=reference_images_dict[patient_id],\n",
    "                                                geometry_info=geometry_info_dict[patient_id], \n",
    "                                                transformer=transformer,\n",
    "                                            )\n",
    "            onsets_positions_pred, cluster_centers_pred = find_min_activation_point_k_means(acti_map=predicted_acti_map,\n",
    "                                                num_onsets = num_onsets,\n",
    "                                                acti_mask=mask_per_patient_dict[patient_id],\n",
    "                                                tetra_links=tetra_links_dict[patient_id], \n",
    "                                                tetra_polydata=mesh_dict[patient_id],\n",
    "                                                ref_img=reference_images_dict[patient_id],\n",
    "                                                geometry_info=geometry_info_dict[patient_id], \n",
    "                                                transformer=transformer,\n",
    "                                            )\n",
    "            total_distance, onsets_positions_ordered, onsets_positions_pred_ordered = \\\n",
    "                compute_closest_points_combination(onsets_positions, onsets_positions_pred)\n",
    "            total_distance_clusters, clusters_positions_ordered, clusters_positions_pred_ordered = \\\n",
    "                                compute_closest_points_combination(cluster_centers, cluster_centers_pred)\n",
    "            sample_out_dict = {\"patient_id\":patient_id, \"onset\":onset,\"num_onsets\":num_onsets, \"file_id\":file_id,\n",
    "                            \"onset_position\":onsets_positions_ordered, \"onset_position_pred\":onsets_positions_pred_ordered, \n",
    "                            \"avg_distance\":total_distance/num_onsets, \"cluster_position\":clusters_positions_ordered,\n",
    "                            \"cluster_position_pred\":clusters_positions_pred_ordered, \"avg_distance_clusters\":total_distance_clusters/num_onsets}\n",
    "            results_list.append(sample_out_dict)\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute onset distance on the test set for different number of employed leads\n",
    "if(cfg.save_plots):\n",
    "    # Initialize structures to perform activation point \n",
    "    patient_ids_list = [f\"Patient{num:03}\" for num in [4, 5, 11, 12, 13, 14, 15]]\n",
    "    mesh_dict = {patient_id:to_tetrahedral(pv.read(\n",
    "        get_inria_small_mesh_path(datapath=cfg.datapath, patient_id=patient_id)\n",
    "    )) for patient_id in patient_ids_list}\n",
    "    geometry_info_dict = get_geometry_info_per_patient_dict(cfg.datapath) # Get geometry information per patient\n",
    "    mask_per_patient_dict = get_masks_per_patient_dict(cfg.datapath) # Get mask for each patient\n",
    "    tetra_links_dict, reference_images_dict = create_tetra_links_and_reference_images(cfg.datapath) #Get surface links and reference images per patient\n",
    "    transformer = sitk.Transform()\n",
    "    transformer.SetIdentity()\n",
    "\n",
    "    if(cfg.model.name in [\"latent-lead\"]):\n",
    "        if(cfg.model.gnn.name!=\"uniform\"):\n",
    "            # Evaluate model for different number of employed leads\n",
    "            removed_leads_range = range(0, 260, 50) #.take(1)\n",
    "            for num_removed_leads in removed_leads_range:\n",
    "                model = reload_best_model_with_leads(num_removed_leads)\n",
    "                # Compute losses of final model on training and test sets\n",
    "                onsets_dict_list = compute_onset_position(in_dataset=test_ds,\n",
    "                                                            mask_per_patient_dict=mask_per_patient_dict,\n",
    "                                                            tetra_links_dict=tetra_links_dict,\n",
    "                                                            mesh_dict=mesh_dict,\n",
    "                                                            reference_images_dict=reference_images_dict,\n",
    "                                                            geometry_info_dict=geometry_info_dict,\n",
    "                                                            transformer=transformer)\n",
    "\n",
    "                # Save aggregated statistics\n",
    "                onset_distances = [d['avg_distance'] for d in onsets_dict_list] # Get onset distances \n",
    "                filename = os.path.join(cfg.model.tb_output_dir, f\"onset_d_dict_{num_removed_leads}\")\n",
    "                onset_dict_to_save = {\"avg_d_min\": np.average(onset_distances),\n",
    "                                    \"std_d_min\": np.std(onset_distances),\n",
    "                                    } \n",
    "                save_dict(onset_dict_to_save,filename)\n",
    "                # Plot histograms\n",
    "                plt.figure()\n",
    "                plt.hist(onset_distances)\n",
    "                plt.xlabel('Distance', fontsize=labelssize)\n",
    "                plt.ylabel('Frequency', fontsize=labelssize)\n",
    "                plt.xticks(fontsize=ticksize)\n",
    "                plt.yticks(fontsize=ticksize)\n",
    "                # plt.title(f'Histogram of Onset Distances (Removed Leads: {num_removed_leads})')\n",
    "                save_png_eps_figure(os.path.join(cfg.model.tb_output_dir, f\"min_onset_d_histogram_{num_removed_leads}\"))\n",
    "                plt.close()\n",
    "                # Save full list of onsets dictionary\n",
    "                save_pickle(onsets_dict_list, filename=f\"onsets_dict_list_{num_removed_leads}\")\n",
    "                # load with: load_pickle(filename=\"onsets_dict_list\")\n",
    "    else:\n",
    "        model = reload_best_model_with_leads()\n",
    "        # Compute losses of final model on training and test sets\n",
    "        onsets_dict_list = compute_onset_position(in_dataset=test_ds,\n",
    "                                                    mask_per_patient_dict=mask_per_patient_dict,\n",
    "                                                    tetra_links_dict=tetra_links_dict,\n",
    "                                                    mesh_dict=mesh_dict,\n",
    "                                                    reference_images_dict=reference_images_dict,\n",
    "                                                    geometry_info_dict=geometry_info_dict,\n",
    "                                                    transformer=transformer)\n",
    "        # Save aggregated statistics\n",
    "        onset_distances = [d['avg_distance'] for d in onsets_dict_list] # Get onset distances \n",
    "        filename = os.path.join(cfg.model.tb_output_dir, f\"onset_d_dict_0\")\n",
    "        onset_dict_to_save = {\"avg_d_min\": np.average(onset_distances),\n",
    "                            \"std_d_min\": np.std(onset_distances),\n",
    "                            } \n",
    "        save_dict(onset_dict_to_save,filename)\n",
    "        save_pickle(onsets_dict_list, filename=f\"onsets_dict_list_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute losses of final model on training and test sets\n",
    "model = reload_best_model_with_leads()\n",
    "train_loss_dict = model.evaluate(train_ds.batch(cfg.batch_size, drop_remainder=True),return_dict=True)\n",
    "filename = os.path.join(cfg.model.tb_output_dir,\"train_loss\")\n",
    "save_dict(train_loss_dict,filename)\n",
    "val_loss_dict = model.evaluate(val_ds.batch(cfg.batch_size, drop_remainder=True),return_dict=True)\n",
    "filename = os.path.join(cfg.model.tb_output_dir,\"val_loss\")\n",
    "save_dict(val_loss_dict,filename)\n",
    "test_loss_dict = model.evaluate(test_ds.batch(cfg.batch_size, drop_remainder=True),return_dict=True)\n",
    "filename = os.path.join(cfg.model.tb_output_dir,\"test_loss\")\n",
    "save_dict(test_loss_dict,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.model.name in [\"latent-lead\"]:\n",
    "    loss_keys = [\"total_loss\", \"acti_loss\", \"signal_loss\", \"mae_acti_loss\", \"alpha_loss\"]\n",
    "elif cfg.model.name in [\"vae-inria\"]:\n",
    "    loss_keys = [\"total_loss\", \"rec_loss\", \"rec_loss_gaus\", \"kl_loss\", \"rec_loss_mae\"]\n",
    "\n",
    "test_loss_stats = compute_average_and_std(test_ds, model, batch_size=cfg.batch_size, metrics=loss_keys)\n",
    "save_pickle(test_loss_stats, filename=\"test_loss_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set completed in configuration file and save it in the experiment folder\n",
    "cfg.completed = True\n",
    "filename = os.path.join(cfg.model.tb_output_dir,\"conf.yaml\")\n",
    "with open(filename,\"w\") as fp:\n",
    "    OmegaConf.save(config=cfg, f=fp)\n",
    "print(\"Script ends\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecgi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
